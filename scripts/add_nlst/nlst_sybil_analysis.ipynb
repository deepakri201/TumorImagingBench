{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a7197c2",
   "metadata": {},
   "source": [
    "# NLST Sybil analysis for Tumor Imaging Bench\n",
    "\n",
    "In this notebook, we provide a demonstration using the Tumor Imaging Bench framework. We utilize the National Lung Screening Trial (NLST) dataset [1], and publicly available tumor bounding boxes from Sybil [2]. We obtain tumor crops using these annotations, and extract embeddings from the set of foundation models. Then, using clincal metadata available from NLST, we train, validate, and test classifiers to predict histology and lung cancer staging. \n",
    "\n",
    "[1] National Lung Screening Trial Research Team. Reduced lung-cancer mortality with low-dose computed tomographic screening. New England Journal of Medicine. 2011 Aug 4;365(5):395-409.\n",
    "\n",
    "[2] Mikhael PG, Wohlwend J, Yala A, Karstens L, Xiang J, Takigami AK, Bourgouin PP, Chan P, Mrah S, Amayri W, Juan YH. Sybil: a validated deep learning model to predict future lung cancer risk from a single low-dose chest computed tomography. Journal of Clinical Oncology. 2023 Apr 20;41(12):2191-200.\n",
    "\n",
    "Deepa Krishnaswamy\n",
    "Brigham and Women's Hospital\n",
    "November 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6961388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Registered extractor: CTClipVitExtractor\n",
      "✓ Registered extractor: CTFMExtractor\n",
      "✓ Registered extractor: FMCIBExtractor\n",
      "Warning: MedImageInsightExtractor not available due to missing dependencies: No module named 'MedImageInsight'\n",
      "✓ Registered extractor: MerlinExtractor\n",
      "✓ Registered extractor: ModelsGenExtractor\n",
      "✓ Registered extractor: PASTAExtractor\n",
      "✓ Registered extractor: SUPREMExtractor\n",
      "✓ Registered extractor: VISTA3DExtractor\n",
      "✓ Registered extractor: VocoExtractor\n",
      "✓ Registered extractor: DummyResNetExtractor\n",
      "✓ Registered extractor: CTClipVitExtractor\n",
      "✓ Registered extractor: CTFMExtractor\n",
      "✓ Registered extractor: FMCIBExtractor\n",
      "Warning: MedImageInsightExtractor not available due to missing dependencies: No module named 'MedImageInsight'\n",
      "✓ Registered extractor: MerlinExtractor\n",
      "✓ Registered extractor: ModelsGenExtractor\n",
      "✓ Registered extractor: PASTAExtractor\n",
      "✓ Registered extractor: SUPREMExtractor\n",
      "✓ Registered extractor: VISTA3DExtractor\n",
      "✓ Registered extractor: VocoExtractor\n",
      "✓ Registered extractor: DummyResNetExtractor\n"
     ]
    }
   ],
   "source": [
    "### Import packages ### \n",
    "\n",
    "import os \n",
    "import sys\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import monai.transforms as monai_transforms\n",
    "import torch\n",
    "from monai.visualize import blend_images\n",
    "\n",
    "import pickle \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from fmcib.visualization import visualize_seed_point\n",
    "\n",
    "sys.path.append(\"/home/exouser/Documents/git/TumorImagingBench/src/tumorimagingbench/\")\n",
    "sys.path.append(\"/home/exouser/Documents/git/TumorImagingBench/src/tumorimagingbench/models\")\n",
    "sys.path.append(\"/home/exouser/Documents/git/TumorImagingBench/src/tumorimagingbench/evaluation\")\n",
    "\n",
    "# from base_feature_extractor import extract_features_for_model, extract_all_features, save_features \n",
    "from base_feature_extractor import save_features \n",
    "from models import CTClipVitExtractor, CTFMExtractor, FMCIBExtractor, MerlinExtractor, ModelsGenExtractor, PASTAExtractor, SUPREMExtractor, VISTA3DExtractor, VocoExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ab9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions ### \n",
    "\n",
    "# We modify the visualize_seed_point from fmcib.visualization, \n",
    "# in order to save a png \n",
    "\n",
    "def visualize_seed_point_save_png(row, output_png_filename):\n",
    "    \"\"\"\n",
    "    This function visualizes a seed point on an image.\n",
    "\n",
    "    Args:\n",
    "        row (pandas.Series): A row containing the information of the seed point, including the image path and the coordinates.\n",
    "            The following columns are expected: \"image_path\", \"coordX\", \"coordY\", \"coordZ\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define the transformation pipeline\n",
    "    is_label_provided = \"label_path\" in row\n",
    "    keys = [\"image_path\", \"label_path\"] if is_label_provided else [\"image_path\"]\n",
    "    all_keys = keys if is_label_provided else [\"image_path\", \"coordX\", \"coordY\", \"coordZ\"]\n",
    "\n",
    "    T = monai_transforms.Compose(\n",
    "        [\n",
    "            monai_transforms.LoadImaged(keys=keys, image_only=True, reader=\"ITKReader\"),\n",
    "            monai_transforms.EnsureChannelFirstd(keys=keys),\n",
    "            monai_transforms.Spacingd(keys=keys, pixdim=1, mode=\"bilinear\", align_corners=True, diagonal=True),\n",
    "            monai_transforms.ScaleIntensityRanged(keys=[\"image_path\"], a_min=-1024, a_max=3072, b_min=0, b_max=1, clip=True),\n",
    "            monai_transforms.Orientationd(keys=keys, axcodes=\"LPS\"),\n",
    "            monai_transforms.SelectItemsd(keys=all_keys),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Apply the transformation pipeline\n",
    "    out = T(row)\n",
    "\n",
    "    # Calculate the center of the image\n",
    "    image = out[\"image_path\"]\n",
    "    if not is_label_provided:\n",
    "        center = (-out[\"coordX\"], -out[\"coordY\"], out[\"coordZ\"])\n",
    "        center = np.linalg.inv(np.array(out[\"image_path\"].affine)) @ np.array(center + (1,))\n",
    "        center = [int(x) for x in center[:3]]\n",
    "\n",
    "        # Define the image and label\n",
    "        label = torch.zeros_like(image)\n",
    "\n",
    "        # Define the dimensions of the image and the patch\n",
    "        C, H, W, D = image.shape\n",
    "        Ph, Pw, Pd = 50, 50, 50\n",
    "\n",
    "        # Calculate and clamp the ranges for cropping\n",
    "        min_h, max_h = max(center[0] - Ph // 2, 0), min(center[0] + Ph // 2, H)\n",
    "        min_w, max_w = max(center[1] - Pw // 2, 0), min(center[1] + Pw // 2, W)\n",
    "        min_d, max_d = max(center[2] - Pd // 2, 0), min(center[2] + Pd // 2, D)\n",
    "\n",
    "        # Check if coordinates are valid\n",
    "        assert min_h < max_h, \"Invalid coordinates: min_h >= max_h\"\n",
    "        assert min_w < max_w, \"Invalid coordinates: min_w >= max_w\"\n",
    "        assert min_d < max_d, \"Invalid coordinates: min_d >= max_d\"\n",
    "\n",
    "        # Define the label for the cropped region\n",
    "        label[:, min_h:max_h, min_w:max_w, min_d:max_d] = 1\n",
    "    else:\n",
    "        label = out[\"label_path\"]\n",
    "        center = torch.nonzero(label).float().mean(dim=0)\n",
    "        center = [int(x) for x in center][1:]\n",
    "\n",
    "    # Blend the image and the label\n",
    "    ret = blend_images(image=image, label=label, alpha=0.3, cmap=\"hsv\", rescale_arrays=False)\n",
    "    ret = ret.permute(3, 2, 1, 0)\n",
    "\n",
    "    # Plot axial slice\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(ret[center[2], :, :])\n",
    "    plt.title(\"Axial\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Plot sagittal slice\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(np.flipud(ret[:, center[1], :]))\n",
    "    plt.title(\"Coronal\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Plot coronal slice\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(np.flipud(ret[:, :, center[0]]))\n",
    "    plt.title(\"Sagittal\")\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "\n",
    "    plt.savefig(output_png_filename)\n",
    "    plt.close()\n",
    "\n",
    "    return\n",
    "\n",
    "def extract_features_for_model_no_split(model_class, get_split_data_fn, preprocess_row_fn):\n",
    "    \"\"\"Extract features for a single model across all splits.\"\"\"\n",
    "    model = model_class()\n",
    "    print(f\"\\nProcessing {model.__class__.__name__}\")\n",
    "    model.load()\n",
    "\n",
    "    model_features = {}\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # for split in [\"train\", \"val\", \"test\"]:\n",
    "        for split in [\"all\"]:\n",
    "            split_df = get_split_data_fn(split)\n",
    "            if split_df is None:\n",
    "                continue\n",
    "\n",
    "            model_features[split] = []\n",
    "\n",
    "            for _, row in tqdm(\n",
    "                split_df.iterrows(), total=len(split_df)\n",
    "            ):\n",
    "                row = preprocess_row_fn(row)\n",
    "                if row is None:\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                image = model.preprocess(row)\n",
    "                image = image.unsqueeze(0)\n",
    "\n",
    "                image = image.to(\"cuda\")\n",
    "                feature = model.forward(image)\n",
    "                if isinstance(feature, torch.Tensor):\n",
    "                    feature = feature.cpu().numpy()\n",
    "                model_features[split].append({\n",
    "                    \"feature\": feature,\n",
    "                    \"row\": row\n",
    "                })\n",
    "\n",
    "    return model_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7221e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set inputs ### \n",
    "\n",
    "create_tumor_csv_files = 0\n",
    "verify_tumor_location = 0\n",
    "extract_all_features = 0\n",
    "train_and_eval_classifiers = 1\n",
    "create_results_figures = 1\n",
    "\n",
    "# De type classification \n",
    "classification_task = \"de_type\"\n",
    "label_type = \"labels_de_type_mapped\"\n",
    "\n",
    "# De stage classification\n",
    "# classification_task = \"de_stag\"\n",
    "# label_type = \"labels_de_stag_mapped\"\n",
    "\n",
    "# For creating the train, val, and test cohorts \n",
    "train_size = 0.6\n",
    "val_size = 0.2\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set filenames/directories ### \n",
    "\n",
    "output_main_directory = \"/home/exouser/Documents/TumorImagingBench/nlst_sybil_analysis\"\n",
    "output_directory = os.path.join(output_main_directory, classification_task)\n",
    "\n",
    "# 1. create_tumor_csv_files \n",
    "# holds the original paths, labels, etc. \n",
    "##### COPY YOUR CSV FILE FROM THE SETUP NOTEBOOK HERE ##### \n",
    "main_csv_filename = os.path.join(output_main_directory, \"nlst_sybil.csv\") \n",
    "\n",
    "# holds in the input nifti files \n",
    "##### COPY YOUR NIFTI FILES HERE ##### \n",
    "nifti_directory = \"/home/exouser/Documents/TumorImagingBench/nlst_data/nifti\" \n",
    "# holds the csv file with the correct paths \n",
    "updated_csv_filename = os.path.join(output_main_directory, \"nlst_sybil_updated_paths.csv\")\n",
    "updated_csv_with_labels_filename = os.path.join(output_main_directory, 'nlst_sybil_updated_paths_with_labels.csv')\n",
    "\n",
    "if not os.path.isdir(output_main_directory):\n",
    "    os.makedirs(output_main_directory,exist_ok=True)\n",
    "\n",
    "# 2. verify_tumor_location \n",
    "tumor_png_directory = os.path.join(output_main_directory, 'verify_tumor_location')\n",
    "if not os.path.isdir(tumor_png_directory):\n",
    "    os.makedirs(tumor_png_directory, exist_ok=True)\n",
    "incorrect_tumor_pngs_filename = os.path.join(output_main_directory, \"incorrect_tumor_pngs.csv\")\n",
    " # incorrect_tumor_pngs_filename = \"/home/exouser/Documents/TumorImagingBench/nlst_sybil_analysis/incorrect_tumor_pngs.csv\"\n",
    "\n",
    "# 3. extract_all_features\n",
    "output_feature_directory = os.path.join(output_directory,\"features\")\n",
    "if not os.path.isdir(output_feature_directory):\n",
    "    os.makedirs(output_feature_directory)\n",
    "\n",
    "# 4. train_and_eval_classifiers \n",
    "output_csv_filename_train = os.path.join(output_directory, \"train.csv\")\n",
    "output_csv_filename_val = os.path.join(output_directory, \"val.csv\")\n",
    "output_csv_filename_test = os.path.join(output_directory, \"test.csv\")\n",
    "CTClipVit_features_filename = os.path.join(output_feature_directory, 'CTClipVit_features.pkl')\n",
    "CTFM_features_filename = os.path.join(output_feature_directory, \"CTFM_features.pkl\")\n",
    "FMCIB_features_filename = os.path.join(output_feature_directory, \"FMCIB_features.pkl\")\n",
    "Merlin_features_filename = os.path.join(output_feature_directory, \"Merlin_features.pkl\")\n",
    "ModelsGen_features_filename = os.path.join(output_feature_directory, \"ModelsGen_features.pkl\")\n",
    "PASTA_features_filename = os.path.join(output_feature_directory, \"PASTA_features.pkl\")\n",
    "SUPREME_features_filename = os.path.join(output_feature_directory, \"SUPREME_features.pkl\")\n",
    "VISTA3D_features_filename = os.path.join(output_feature_directory, \"VISTA3D_features.pkl\")\n",
    "Voco_features_filename = os.path.join(output_feature_directory, \"Voco_features.pkl\")\n",
    "\n",
    "# 5. create_results_figures \n",
    "metrics_directory = os.path.join(output_directory, \"metrics\")\n",
    "roc_directory = os.path.join(metrics_directory, \"roc\")\n",
    "scores_directory = os.path.join(metrics_directory, \"scores\")\n",
    "if not os.path.isdir(metrics_directory):\n",
    "    os.makedirs(metrics_directory, exist_ok=True)\n",
    "if not os.path.isdir(roc_directory):\n",
    "    os.makedirs(roc_directory,exist_ok=True)\n",
    "if not os.path.isdir(scores_directory):\n",
    "    os.makedirs(scores_directory, exist_ok=True)\n",
    "# filenames for the results \n",
    "CTClipVit_scores_filename = os.path.join(scores_directory, 'CTClipVit_features.npz')\n",
    "CTFM_scores_filename = os.path.join(scores_directory, \"CTFM_features.npz\")\n",
    "FMCIB_scores_filename = os.path.join(scores_directory, \"FMCIB_features.npz\")\n",
    "Merlin_scores_filename = os.path.join(scores_directory, \"Merlin_features.npz\")\n",
    "ModelsGen_scores_filename = os.path.join(scores_directory, \"ModelsGen_features.npz\")\n",
    "PASTA_scores_filename = os.path.join(scores_directory, \"PASTA_features.npz\")\n",
    "SUPREME_scores_filename = os.path.join(scores_directory, \"SUPREME_features.npz\")\n",
    "VISTA3D_scores_filename = os.path.join(scores_directory, \"VISTA3D_features.npz\")\n",
    "Voco_scores_filename = os.path.join(scores_directory, \"Voco_features.npz\")\n",
    "# accuracy filename \n",
    "output_scores_png_filename = os.path.join(metrics_directory, 'accuracy_over_models.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63335a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Skipping creation of tumor csv file ****\n"
     ]
    }
   ],
   "source": [
    "### 1. Create tumor csv files ### \n",
    "\n",
    "if (create_tumor_csv_files): \n",
    "\n",
    "    print('**** Create tumor csv file ****')\n",
    "\n",
    "    # Load dataframe \n",
    "    print('Reading original csv file: ' + str(main_csv_filename))\n",
    "    df_main = pd.read_csv(main_csv_filename)\n",
    "    df_output = df_main.copy(deep=True) \n",
    "\n",
    "    # Create new image paths \n",
    "    image_paths = df_output['image_path'].values\n",
    "    image_paths_filenames = [os.path.basename(f) for f in image_paths]\n",
    "    image_paths_new = [os.path.join(nifti_directory,f) for f in image_paths_filenames]\n",
    "    df_output['image_path'] = image_paths_new \n",
    "\n",
    "    # Save the csv with the modified paths\n",
    "    print('Writing csv with updated paths: ' + str(updated_csv_filename)) \n",
    "    df_output.to_csv(updated_csv_filename)\n",
    "\n",
    "else: \n",
    "\n",
    "    print('**** Skipping creation of tumor csv file ****')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf240619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Skipping verification of tumor location ****\n"
     ]
    }
   ],
   "source": [
    "### 2. Verify tumor location ### \n",
    "# Save out pngs for all to verify \n",
    "\n",
    "if (verify_tumor_location):\n",
    "\n",
    "    print('**** Verify tumor location ****')\n",
    "\n",
    "    print('Reading csv with updated paths: ' + str(updated_csv_filename))\n",
    "    df_for_csv = pd.read_csv(updated_csv_filename)\n",
    "    num_tumors = len(df_for_csv)\n",
    "    print('num_tumors: ' + str(num_tumors))\n",
    "\n",
    "    checkpoints = {int(num_tumors * i / 10) for i in range(1, 11)}\n",
    "\n",
    "    for index in range(0,num_tumors): \n",
    "        row = dict(df_for_csv.iloc[index])\n",
    "        # row = pd.Series(row)\n",
    "        SOPInstanceUID = row['SOPInstanceUID']\n",
    "        output_png_filename = os.path.join(tumor_png_directory, str(SOPInstanceUID) + \".png\")\n",
    "        visualize_seed_point_save_png(row, output_png_filename)\n",
    "        if index in checkpoints:\n",
    "            print(f\"{(index / num_tumors) * 100:.0f}% of tumors processed.\")\n",
    "\n",
    "else: \n",
    "\n",
    "    print('**** Skipping verification of tumor location ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7799a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Skipping extraction of all features ****\n"
     ]
    }
   ],
   "source": [
    "### 3. Extract all features ### \n",
    "# Since the feature extraction takes a long time, we do it once and save the pickle files\n",
    "# Only when we train/eval classifiers do we split the data \n",
    "\n",
    "if (extract_all_features):\n",
    "\n",
    "    print('**** Extracting all features ****')\n",
    "\n",
    "    def get_split_data_fn(split):\n",
    "        \"\"\"Get dataset split.\"\"\"\n",
    "        split_paths = {\n",
    "            \"all\": updated_csv_filename,\n",
    "        }\n",
    "        if split not in split_paths:\n",
    "            raise ValueError(f\"Invalid split: {split}\")\n",
    "        return pd.read_csv(split_paths[split])\n",
    "\n",
    "    def preprocess_row_fn(row):\n",
    "        \"\"\"Preprocess a row from the dataset.\"\"\"\n",
    "        return row\n",
    "\n",
    "    ### Processing ###\n",
    "\n",
    "    model_classes = [CTClipVitExtractor,\n",
    "                     CTFMExtractor, \n",
    "                     FMCIBExtractor, \n",
    "                     MerlinExtractor, \n",
    "                     ModelsGenExtractor, \n",
    "                     PASTAExtractor, \n",
    "                     SUPREMExtractor, \n",
    "                     VISTA3DExtractor, \n",
    "                     VocoExtractor] \n",
    "    model_classes_names = ['CTClipVit', 'CTFM', 'FMCIB', 'Merlin', 'ModelsGen', 'PASTA', 'SUPREME', 'VISTA3D', 'Voco']\n",
    "\n",
    "    for model_class, model_class_name in zip(model_classes, model_classes_names):\n",
    "        try:\n",
    "            features = extract_features_for_model_no_split(model_class, get_split_data_fn, preprocess_row_fn)\n",
    "            output_filename = os.path.join(output_feature_directory, model_class_name + '_features.pkl')\n",
    "            save_features(features, output_filename) \n",
    "        except: \n",
    "            print('Cannot extract features from model')\n",
    "\n",
    "else: \n",
    "\n",
    "    print('**** Skipping extraction of all features ****')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Remove tumors with images that have reconstruction problems/missing frames, etc ****\n",
      "Num tumors to remove: 42\n"
     ]
    }
   ],
   "source": [
    "### 3b. Load the csv file with SOPs to remove ### \n",
    "# These are due to irregular pixel spacing, not a reconstructable 3D volume, missing frames, incorrect orientation, etc. \n",
    "\n",
    "if (train_and_eval_classifiers):\n",
    "    \n",
    "    print('**** Remove tumors with images that have reconstruction problems/missing frames, etc ****')\n",
    "     \n",
    "    incorrect_tumor_pngs_df = pd.read_csv(incorrect_tumor_pngs_filename)\n",
    "    # Get a list of the ones to remove \n",
    "    remove_tumors_df = incorrect_tumor_pngs_df[incorrect_tumor_pngs_df['keep_tumor']==0]\n",
    "    remove_tumor_sops = sorted(list(set(remove_tumors_df['SOPInstanceUID'].values)))\n",
    "    print('Num tumors to remove: ' + str(len(remove_tumor_sops))) \n",
    "\n",
    "else: \n",
    "\n",
    "    print('**** Skipping the train and eval of classifiers ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a06ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Train and eval classifiers ****\n",
      "classification_task: de_type\n",
      "Num tumors of original: 734\n",
      "Num tumors after removal of problematic images: 692\n",
      "num_patients: 271\n",
      "label0_number: 180\n",
      "label1_number: 91\n",
      "num_train_patients_label0: 108\n",
      "num_train_patients_label1: 54\n",
      "num_val_patients_label0: 36\n",
      "num_val_patients_label1: 18\n",
      "num_test_patients_label0: 36\n",
      "num_test_patients_label1: 19\n",
      "271\n",
      "num_train_patients: 162\n",
      "num_val_patients: 54\n",
      "num_test_patients: 55\n",
      "Train size: 234\n",
      "Val size: 84\n",
      "Test size: 92\n",
      "train_X: (234, 512)\n",
      "val_X: (84, 512)\n",
      "test_X: (92, 512)\n",
      "train_y: 234\n",
      "val_y: 84\n",
      "test_y: 92\n",
      "train_y unique: [0 1]\n",
      "val_y unique: [0 1]\n",
      "test_y unique: [0 1]\n",
      "C = 0.0001: Validation accuracy = 0.5471186440677966\n",
      "C = 0.001: Validation accuracy = 0.5464406779661017\n",
      "C = 0.01: Validation accuracy = 0.5471186440677966\n",
      "C = 0.1: Validation accuracy = 0.5471186440677966\n",
      "C = 1: Validation accuracy = 0.544406779661017\n",
      "C = 10: Validation accuracy = 0.5586440677966101\n",
      "C = 100: Validation accuracy = 0.5986440677966102\n",
      "Best Validation accuracy: 0.5986440677966102\n",
      "Score on the testing data: 0.5578347578347578\n",
      "train_X: (234, 512)\n",
      "val_X: (84, 512)\n",
      "test_X: (92, 512)\n",
      "train_y: 234\n",
      "val_y: 84\n",
      "test_y: 92\n",
      "train_y unique: [0 1]\n",
      "val_y unique: [0 1]\n",
      "test_y unique: [0 1]\n",
      "C = 0.0001: Validation accuracy = 0.4542372881355932\n",
      "C = 0.001: Validation accuracy = 0.4244067796610169\n",
      "C = 0.01: Validation accuracy = 0.43322033898305085\n",
      "C = 0.1: Validation accuracy = 0.4338983050847458\n",
      "C = 1: Validation accuracy = 0.4345762711864407\n",
      "C = 10: Validation accuracy = 0.4345762711864407\n",
      "C = 100: Validation accuracy = 0.44169491525423726\n",
      "Best Validation accuracy: 0.4542372881355932\n",
      "Score on the testing data: 0.45982905982905986\n",
      "train_X: (234, 4096)\n",
      "val_X: (84, 4096)\n",
      "test_X: (92, 4096)\n",
      "train_y: 234\n",
      "val_y: 84\n",
      "test_y: 92\n",
      "train_y unique: [0 1]\n",
      "val_y unique: [0 1]\n",
      "test_y unique: [0 1]\n",
      "C = 0.0001: Validation accuracy = 0.47525423728813565\n",
      "C = 0.001: Validation accuracy = 0.44881355932203393\n",
      "C = 0.01: Validation accuracy = 0.4630508474576271\n",
      "C = 0.1: Validation accuracy = 0.49898305084745764\n",
      "C = 1: Validation accuracy = 0.5118644067796609\n",
      "C = 10: Validation accuracy = 0.49694915254237293\n",
      "C = 100: Validation accuracy = 0.512542372881356\n",
      "Best Validation accuracy: 0.512542372881356\n",
      "Score on the testing data: 0.6074074074074074\n",
      "train_X: (234, 512)\n",
      "val_X: (84, 512)\n",
      "test_X: (92, 512)\n",
      "train_y: 234\n",
      "val_y: 84\n",
      "test_y: 92\n",
      "train_y unique: [0 1]\n",
      "val_y unique: [0 1]\n",
      "test_y unique: [0 1]\n",
      "C = 0.0001: Validation accuracy = 0.6555932203389832\n",
      "C = 0.001: Validation accuracy = 0.6555932203389832\n",
      "C = 0.01: Validation accuracy = 0.656949152542373\n",
      "C = 0.1: Validation accuracy = 0.6501694915254238\n",
      "C = 1: Validation accuracy = 0.6366101694915254\n",
      "C = 10: Validation accuracy = 0.6149152542372881\n",
      "C = 100: Validation accuracy = 0.5403389830508475\n",
      "Best Validation accuracy: 0.656949152542373\n",
      "Score on the testing data: 0.5356125356125355\n",
      "train_X: (234, 4096)\n",
      "val_X: (84, 4096)\n",
      "test_X: (92, 4096)\n",
      "train_y: 234\n",
      "val_y: 84\n",
      "test_y: 92\n",
      "train_y unique: [0 1]\n",
      "val_y unique: [0 1]\n",
      "test_y unique: [0 1]\n",
      "C = 0.0001: Validation accuracy = 0.543728813559322\n",
      "C = 0.001: Validation accuracy = 0.5484745762711865\n",
      "C = 0.01: Validation accuracy = 0.5254237288135593\n",
      "C = 0.1: Validation accuracy = 0.5254237288135594\n",
      "C = 1: Validation accuracy = 0.5294915254237288\n",
      "C = 10: Validation accuracy = 0.5335593220338983\n",
      "C = 100: Validation accuracy = 0.512542372881356\n",
      "Best Validation accuracy: 0.5484745762711865\n",
      "Score on the testing data: 0.593162393162393\n",
      "train_X: (234, 1024)\n",
      "val_X: (84, 1024)\n",
      "test_X: (92, 1024)\n",
      "train_y: 234\n",
      "val_y: 84\n",
      "test_y: 92\n",
      "train_y unique: [0 1]\n",
      "val_y unique: [0 1]\n",
      "test_y unique: [0 1]\n",
      "C = 0.0001: Validation accuracy = 0.5288135593220339\n",
      "C = 0.001: Validation accuracy = 0.5322033898305085\n",
      "C = 0.01: Validation accuracy = 0.5247457627118643\n",
      "C = 0.1: Validation accuracy = 0.528135593220339\n",
      "C = 1: Validation accuracy = 0.528135593220339\n",
      "C = 10: Validation accuracy = 0.5315254237288136\n",
      "C = 100: Validation accuracy = 0.5403389830508474\n",
      "Best Validation accuracy: 0.5403389830508474\n",
      "Score on the testing data: 0.5105413105413105\n",
      "train_X: (234, 512)\n",
      "val_X: (84, 512)\n",
      "test_X: (92, 512)\n",
      "train_y: 234\n",
      "val_y: 84\n",
      "test_y: 92\n",
      "train_y unique: [0 1]\n",
      "val_y unique: [0 1]\n",
      "test_y unique: [0 1]\n",
      "C = 0.0001: Validation accuracy = 0.5803389830508475\n",
      "C = 0.001: Validation accuracy = 0.5803389830508475\n",
      "C = 0.01: Validation accuracy = 0.5810169491525424\n",
      "C = 0.1: Validation accuracy = 0.5810169491525424\n",
      "C = 1: Validation accuracy = 0.576271186440678\n",
      "C = 10: Validation accuracy = 0.5383050847457627\n",
      "C = 100: Validation accuracy = 0.5722033898305086\n",
      "Best Validation accuracy: 0.5810169491525424\n",
      "Score on the testing data: 0.6017094017094017\n",
      "train_X: (234, 768)\n",
      "val_X: (84, 768)\n",
      "test_X: (92, 768)\n",
      "train_y: 234\n",
      "val_y: 84\n",
      "test_y: 92\n",
      "train_y unique: [0 1]\n",
      "val_y unique: [0 1]\n",
      "test_y unique: [0 1]\n",
      "C = 0.0001: Validation accuracy = 0.47796610169491527\n",
      "C = 0.001: Validation accuracy = 0.4644067796610169\n",
      "C = 0.01: Validation accuracy = 0.4705084745762712\n",
      "C = 0.1: Validation accuracy = 0.4813559322033898\n",
      "C = 1: Validation accuracy = 0.4840677966101695\n",
      "C = 10: Validation accuracy = 0.472542372881356\n",
      "C = 100: Validation accuracy = 0.42847457627118646\n",
      "Best Validation accuracy: 0.4840677966101695\n",
      "Score on the testing data: 0.5333333333333333\n",
      "train_X: (234, 3072)\n",
      "val_X: (84, 3072)\n",
      "test_X: (92, 3072)\n",
      "train_y: 234\n",
      "val_y: 84\n",
      "test_y: 92\n",
      "train_y unique: [0 1]\n",
      "val_y unique: [0 1]\n",
      "test_y unique: [0 1]\n",
      "C = 0.0001: Validation accuracy = 0.4576271186440678\n",
      "C = 0.001: Validation accuracy = 0.4528813559322034\n",
      "C = 0.01: Validation accuracy = 0.4576271186440678\n",
      "C = 0.1: Validation accuracy = 0.4576271186440678\n",
      "C = 1: Validation accuracy = 0.4576271186440678\n",
      "C = 10: Validation accuracy = 0.4576271186440678\n",
      "C = 100: Validation accuracy = 0.4576271186440678\n",
      "Best Validation accuracy: 0.4576271186440678\n",
      "Score on the testing data: 0.45128205128205123\n"
     ]
    }
   ],
   "source": [
    "### 4. Train and eval classifiers ### \n",
    "# Only here do we divide into train, val, and test cohorts \n",
    "\n",
    "if (train_and_eval_classifiers):\n",
    "\n",
    "    print('**** Train and eval classifiers ****') \n",
    "    print('classification_task: ' + str(classification_task))\n",
    "\n",
    "    # Set the labels depending on the classification_task\n",
    "    if (classification_task==\"de_type\"): \n",
    "        col_type = \"labels_de_type_mapped\"\n",
    "        labels_map = {\n",
    "                    \"Adenocarcinoma, NOS\": 0, \n",
    "                    \"Squamous cell carcinoma, NOS\": 1\n",
    "                    }\n",
    "    elif (classification_task==\"de_stag\"): \n",
    "        col_type = \"labels_de_stag_mapped\"\n",
    "\n",
    "        labels_map = {\n",
    "                    0: 0, # stage IA\n",
    "                    1: 0, # stage 1B\n",
    "                    2: 0, # stage IIA\n",
    "                    3: 0, # stage IIB\n",
    "                    4: 1, # stage IIIA\n",
    "                    5: 1, # stage IIB\n",
    "                    6: 1  # stage IV \n",
    "                    }\n",
    "    \n",
    "    ############################\n",
    "    ### Processing of labels ### \n",
    "    ############################\n",
    "\n",
    "    # Read in the csv files that contains the labels - could also get from features pkl files \n",
    "    df_output = pd.read_csv(updated_csv_filename)\n",
    "    print('Num tumors of original: ' + str(len(df_output)))\n",
    "\n",
    "    # Now remove the sops that have reconstruction problems\n",
    "    df_output = df_output[~df_output['SOPInstanceUID'].isin(remove_tumor_sops)]\n",
    "    print('Num tumors after removal of problematic images: ' + str(len(df_output)))\n",
    "\n",
    "    # Keep only certain labels         \n",
    "    labels_keep = labels_map.keys()\n",
    "    df_output = df_output[df_output[col_type].isin(labels_keep)]\n",
    "    # Create a new \"labels\" column \n",
    "    df_output[\"label\"] = df_output[col_type].map(labels_map)\n",
    "    # Save as csv - backup  \n",
    "    df_output.to_csv(updated_csv_with_labels_filename)\n",
    "\n",
    "    ##################################\n",
    "    ### Divide into train/val/test ###\n",
    "    ################################## \n",
    "\n",
    "    ### Naive division of patients ### \n",
    "    # patients = sorted(list(set(df_output['PatientID'].values)))\n",
    "    # num_patients = len(patients)\n",
    "    # num_train_patients = np.int32(np.floor(num_patients * train_size))\n",
    "    # num_val_patients = np.int32(np.floor(num_patients * val_size))\n",
    "    # train_patients = patients[0:num_train_patients]\n",
    "    # val_patients = patients[num_train_patients:num_train_patients+num_val_patients]\n",
    "    # test_patients = patients[num_train_patients+num_val_patients::]\n",
    "\n",
    "    ### Divide patients with equal distributions of labels ### \n",
    "\n",
    "    # First get the number of original patients\n",
    "    patients = sorted(list(set(df_output['PatientID'].values)))\n",
    "    num_patients = len(patients)\n",
    "    print('num_patients: ' + str(num_patients))\n",
    "    # Create temp df with one row per patient\n",
    "    temp_df = df_output.copy(deep=True)\n",
    "    temp_df = temp_df[['PatientID', 'label']]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    # Get label counts \n",
    "    label_counts_df = temp_df['label'].value_counts()\n",
    "    label0_number = label_counts_df.values[0]\n",
    "    label1_number = label_counts_df.values[1]\n",
    "    # Get PatientIDs for each label\n",
    "    PatientIDs_label0 = sorted(temp_df[temp_df['label']==0]['PatientID'].values)\n",
    "    PatientIDs_label1 = sorted(temp_df[temp_df['label']==1]['PatientID'].values)\n",
    "    print('label0_number: ' + str(label0_number))\n",
    "    print('label1_number: ' + str(label1_number))\n",
    "    # Make sure these two don't overlap \n",
    "    patient_intersect = sorted(list(set(PatientIDs_label0) & set(PatientIDs_label1)))\n",
    "    if len(patient_intersect)>0:\n",
    "        print('patient_intersect should be 0: ' + str(patient_intersect))\n",
    "        print('ERROR: FIX THE PATIENT SPLIT')\n",
    "\n",
    "    # Now divide patients \n",
    "    # Train\n",
    "    num_train_patients_label0 = np.int32(np.floor(train_size * label0_number))\n",
    "    num_train_patients_label1 = np.int32(np.floor(train_size * label1_number))\n",
    "    # Val\n",
    "    num_val_patients_label0 = np.int32(np.floor(val_size * label0_number))\n",
    "    num_val_patients_label1 = np.int32(np.floor(val_size * label1_number))\n",
    "    # Test\n",
    "    num_test_patients_label0 = np.int32(np.floor(test_size * label0_number))\n",
    "    # num_test_patients_label1 = np.int32(np.floor(test_size * label1_number))\n",
    "    num_test_patients_label1 = num_patients - (num_train_patients_label0 + num_train_patients_label1 +\n",
    "                                               num_val_patients_label0 + num_val_patients_label1 +\n",
    "                                               num_test_patients_label0)\n",
    "    print('num_train_patients_label0: ' + str(num_train_patients_label0))\n",
    "    print('num_train_patients_label1: ' + str(num_train_patients_label1))\n",
    "    print('num_val_patients_label0: ' + str(num_val_patients_label0))\n",
    "    print('num_val_patients_label1: ' + str(num_val_patients_label1))\n",
    "    print('num_test_patients_label0: ' + str(num_test_patients_label0))\n",
    "    print('num_test_patients_label1: ' + str(num_test_patients_label1))\n",
    "    print(num_train_patients_label0 + num_train_patients_label1 +\n",
    "          num_val_patients_label0 + num_val_patients_label1 +\n",
    "          num_test_patients_label0 + num_test_patients_label1)\n",
    "    \n",
    "    # Get the PatientIDs\n",
    "    train_patients = PatientIDs_label0[0:num_train_patients_label0] + \\\n",
    "                     PatientIDs_label1[0:num_train_patients_label1]\n",
    "    val_patients = PatientIDs_label0[num_train_patients_label0:num_train_patients_label0 + num_val_patients_label0] + \\\n",
    "                   PatientIDs_label1[num_train_patients_label1:num_train_patients_label1 + num_val_patients_label1]\n",
    "    test_patients = PatientIDs_label0[num_train_patients_label0 + num_val_patients_label0::] + \\\n",
    "                    PatientIDs_label1[num_train_patients_label1 + num_val_patients_label1::]\n",
    "\n",
    "    # Create the dataframes \n",
    "    df_train = df_output[df_output['PatientID'].isin(train_patients)]\n",
    "    df_val = df_output[df_output['PatientID'].isin(val_patients)]\n",
    "    df_test = df_output[df_output['PatientID'].isin(test_patients)]\n",
    "    df_train.to_csv(output_csv_filename_train)\n",
    "    df_val.to_csv(output_csv_filename_val)\n",
    "    df_test.to_csv(output_csv_filename_test)\n",
    "\n",
    "    print('num_train_patients: ' + str(len(train_patients)))\n",
    "    print('num_val_patients: ' + str(len(val_patients)))\n",
    "    print('num_test_patients: ' + str(len(test_patients)))\n",
    "\n",
    "    print('Train size: ' + str(len(df_train)))\n",
    "    print('Val size: ' + str(len(df_val)))\n",
    "    print('Test size: ' + str(len(df_test)))\n",
    "\n",
    "    # Get the SOPInstanceUIDs - as the feature dfs contain extra \n",
    "    train_sops = list(set(df_train['SOPInstanceUID'].values))\n",
    "    val_sops = list(set(df_val['SOPInstanceUID'].values))\n",
    "    test_sops = list(set(df_test['SOPInstanceUID'].values))\n",
    "\n",
    "    ##################################\n",
    "    ### Train/val/test classifiers ###\n",
    "    ##################################\n",
    "     \n",
    "    features_filename_list = [CTClipVit_features_filename,\n",
    "                              CTFM_features_filename, \n",
    "                              FMCIB_features_filename, \n",
    "                              Merlin_features_filename,\n",
    "                              ModelsGen_features_filename,\n",
    "                              PASTA_features_filename,\n",
    "                              SUPREME_features_filename,\n",
    "                              VISTA3D_features_filename,\n",
    "                              Voco_features_filename]\n",
    "\n",
    "    for features_filename in features_filename_list: \n",
    "\n",
    "        with open(features_filename, 'rb') as f: \n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        ### Original ###\n",
    "        # # get features and concatenate \n",
    "        # train_X = [data['train'][i]['feature'] for i in range(len(data['train']))]\n",
    "        # train_X = np.concatenate(train_X,axis=0)\n",
    "        # val_X = [data['val'][i]['feature'] for i in range(len(data['val']))]\n",
    "        # val_X = np.concatenate(val_X,axis=0)\n",
    "        # test_X = [data['test'][i]['feature'] for i in range(len(data['test']))]\n",
    "        # test_X = np.concatenate(test_X,axis=0)\n",
    "        # # get labels\n",
    "        # \n",
    "        # val_y = [data['val'][i]['row']['label'] for i in range(len(data['val']))]\n",
    "        # test_y = [data['test'][i]['row']['label'] for i in range(len(data['test']))]    \n",
    "\n",
    "        ### Now filter by SOPInstanceUID instead of Patient ### \n",
    "        # get data \n",
    "        all_X = [data['all'][i]['feature'] for i in range(len(data['all']))]\n",
    "        train_X = [entry['feature'] for entry in data['all'] if entry['row']['SOPInstanceUID'] in train_sops]\n",
    "        train_X = np.concatenate(train_X,axis=0)\n",
    "        val_X = [entry['feature'] for entry in data['all'] if entry['row']['SOPInstanceUID'] in val_sops]\n",
    "        val_X = np.concatenate(val_X,axis=0)\n",
    "        test_X = [entry['feature'] for entry in data['all'] if entry['row']['SOPInstanceUID'] in test_sops]\n",
    "        test_X = np.concatenate(test_X,axis=0)\n",
    "        print('train_X: ' + str(train_X.shape))\n",
    "        print('val_X: ' + str(val_X.shape))\n",
    "        print('test_X: ' + str(test_X.shape))\n",
    "\n",
    "        # get labels \n",
    "        all_y = [data['all'][i]['row'][label_type] for i in range(len(data['all']))]\n",
    "        train_y = [entry['row'][label_type] for entry in data['all'] if entry['row']['SOPInstanceUID'] in train_sops]      \n",
    "        val_y =  [entry['row'][label_type] for entry in data['all'] if entry['row']['SOPInstanceUID'] in val_sops]  \n",
    "        test_y = [entry['row'][label_type] for entry in data['all'] if entry['row']['SOPInstanceUID'] in test_sops]  \n",
    "        print('train_y: ' + str(len(train_y)))\n",
    "        print('val_y: ' + str(len(val_y)))\n",
    "        print('test_y: ' + str(len(test_y)))\n",
    "        # map the labels \n",
    "        train_y = [labels_map[k] for k in train_y]\n",
    "        val_y = [labels_map[k] for k in val_y]\n",
    "        test_y = [labels_map[k] for k in test_y]\n",
    "        print('train_y unique: ' + str(np.unique(train_y)))\n",
    "        print('val_y unique: ' + str(np.unique(val_y)))\n",
    "        print('test_y unique: ' + str(np.unique(test_y)))\n",
    "        \n",
    "        # ROC curves filename \n",
    "        fm_type = os.path.basename(features_filename)\n",
    "        fm_type = Path(fm_type).stem\n",
    "        output_png_filename = os.path.join(roc_directory, fm_type + '.png')\n",
    "        \n",
    "        # ROC measures filename \n",
    "        output_npz_filename = os.path.join(scores_directory, fm_type + \".npz\")\n",
    "\n",
    "        # Training loop with simple hyperparameter search using validation set\n",
    "        C_range = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "        best_val_score = 0\n",
    "        best_model = None\n",
    "\n",
    "        for C in C_range:\n",
    "            linear_model = LogisticRegression(C=C, max_iter=1000)\n",
    "            linear_model.fit(train_X, train_y)\n",
    "            val_pred = linear_model.predict_proba(val_X)[:, 1]\n",
    "            val_score = roc_auc_score(val_y, val_pred)\n",
    "\n",
    "            print(f\"C = {C}: Validation accuracy = {val_score}\")\n",
    "\n",
    "            # Keep track of the best model\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                best_model = linear_model\n",
    "\n",
    "        print(f\"Best Validation accuracy: {best_val_score}\")\n",
    "\n",
    "        # Test \n",
    "        test_pred = best_model.predict_proba(test_X)[:, 1]\n",
    "        test_score = roc_auc_score(test_y, test_pred)\n",
    "        print(f\"Score on the testing data: {test_score}\")\n",
    "\n",
    "        # Plot curves \n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "\n",
    "        split_map = {\n",
    "            \"Train\": [train_X, train_y, \"steelblue\"],\n",
    "            \"Val\": [val_X, val_y, \"lightblue\"],\n",
    "            \"Test\": [test_X, test_y, \"darkblue\"]\n",
    "        }\n",
    "\n",
    "        roc_values = [] \n",
    "        for split in [\"Train\", \"Val\", \"Test\"]:\n",
    "            feats, label, color = split_map[split]\n",
    "            fpr, tpr, thresholds = roc_curve(label, best_model.predict_proba(feats)[:, 1])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            roc_values.append(roc_auc)\n",
    "            plt.plot(fpr, tpr, color=color, lw=lw, label=f'{split} ROC curve (area = %0.2f)' % roc_auc, alpha=0.8)\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], color='gray', lw=lw, linestyle='--', alpha=0.6)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        # plt.show()\n",
    "        plt.savefig(output_png_filename)\n",
    "        plt.close() \n",
    "        \n",
    "        # Save npz file \n",
    "        np.savez(output_npz_filename, score=test_score, roc_values=roc_values) \n",
    "\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('**** Skipping the train and eval of classifiers ****')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7453b0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Create results figures ****\n"
     ]
    }
   ],
   "source": [
    "### 5. Create results figures ### \n",
    "\n",
    "if (create_results_figures):\n",
    "\n",
    "    print('**** Create results figures ****')\n",
    "\n",
    "    ### Load data ### \n",
    "\n",
    "    scores_filename_list = [CTClipVit_scores_filename,\n",
    "                            CTFM_scores_filename, \n",
    "                            FMCIB_scores_filename, \n",
    "                            Merlin_scores_filename,\n",
    "                            ModelsGen_scores_filename,\n",
    "                            PASTA_scores_filename,\n",
    "                            SUPREME_scores_filename,\n",
    "                            VISTA3D_scores_filename,\n",
    "                            Voco_scores_filename]\n",
    "    fm_models = [os.path.basename(f) for f in scores_filename_list] \n",
    "    fm_models = [Path(f).stem for f in fm_models]\n",
    "    fm_models = [f.split('_')[0] for f in fm_models]\n",
    "\n",
    "    scores = [] \n",
    "    for scores_filename in scores_filename_list: \n",
    "        data = np.load(scores_filename)['score']\n",
    "        scores.append(data)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['FM'] = fm_models \n",
    "    df['test_accuracy'] = scores \n",
    "\n",
    "    ### Create auc plot ###\n",
    "\n",
    "    fig = px.bar(df, x='FM', y='test_accuracy', title='Test accuracy per foundation model')\n",
    "    fig.write_image(output_scores_png_filename)\n",
    "\n",
    "else:\n",
    "\n",
    "    print('**** Skipping creation of results figures ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2da3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumorimagingbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
