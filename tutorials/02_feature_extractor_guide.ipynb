{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor Guide\n",
    "\n",
    "How to create a new dataset feature extractor for TumorImagingBench.\n",
    "\n",
    "This notebook demonstrates the complete workflow for adding a new dataset to TumorImagingBench."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The TumorImagingBench framework uses **dataset extractors** to integrate new datasets. Each extractor is a Python module that:\n",
    "\n",
    "1. **Loads dataset splits** (train/val/test) from CSV files\n",
    "2. **Validates and preprocesses** each sample\n",
    "3. **Extracts features** using all available foundation models\n",
    "4. **Saves results** as pickle files\n",
    "\n",
    "The benefit is a unified interface for feature extraction across all models and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dataset Preparation\n",
    "\n",
    "Before creating a feature extractor, prepare your dataset in CSV format.\n",
    "\n",
    "### Expected Directory Structure\n",
    "\n",
    "```\n",
    "data/eval/my_dataset/\n",
    "├── train.csv\n",
    "├── val.csv\n",
    "├── test.csv\n",
    "└── images/\n",
    "    ├── scan_001.nii.gz\n",
    "    ├── scan_002.nii.gz\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "### CSV Format\n",
    "\n",
    "Each CSV file (train.csv, val.csv, test.csv) should have:\n",
    "\n",
    "**Required columns:**\n",
    "- `image_path` (str): Absolute path to NIFTI file (.nii.gz)\n",
    "- `coordX` (float): X centroid coordinate in physical space (mm)\n",
    "- `coordY` (float): Y centroid coordinate in physical space (mm)\n",
    "- `coordZ` (float): Z centroid coordinate in physical space (mm)\n",
    "\n",
    "**Optional columns:**\n",
    "- `label` (int/float): Target variable for classification/regression\n",
    "- Any other metadata (patient_id, scan_date, etc.)\n",
    "\n",
    "### Example CSV\n",
    "\n",
    "```csv\n",
    "image_path,coordX,coordY,coordZ,label\n",
    "/path/to/data/eval/my_dataset/images/scan_001.nii.gz,100.5,150.3,200.1,0\n",
    "/path/to/data/eval/my_dataset/images/scan_002.nii.gz,110.2,160.8,210.5,1\n",
    "/path/to/data/eval/my_dataset/images/scan_003.nii.gz,95.8,145.2,195.3,0\n",
    "```\n",
    "\n",
    "**Important Notes:**\n",
    "- All paths must be absolute paths (or relative to the working directory)\n",
    "- Coordinates should be in physical space (millimeters), not voxel indices\n",
    "- The coordinates represent the centroid of the region of interest (lesion, tumor, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create the Feature Extractor\n",
    "\n",
    "Now we'll create the Python module for feature extraction. There are four key components:\n",
    "\n",
    "1. `get_split_data(split, ...)` - Load dataset split from CSV\n",
    "2. `preprocess_row(row)` - Validate and preprocess each sample\n",
    "3. `extract_features(...)` - Main entry point\n",
    "4. Command-line interface with argparse\n",
    "\n",
    "Let's implement each one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1: get_split_data()\n",
    "\n",
    "This function loads and returns the dataset split as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Function 'get_split_data' defined\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_split_data(split, train_csv, val_csv, test_csv):\n",
    "    \"\"\"\n",
    "    Load dataset split from CSV.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    split : str\n",
    "        One of ['train', 'val', 'test']\n",
    "    train_csv : str\n",
    "        Path to training CSV\n",
    "    val_csv : str\n",
    "        Path to validation CSV\n",
    "    test_csv : str\n",
    "        Path to test CSV\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: image_path, coordX, coordY, coordZ, label (optional), ...\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If split is not recognized\n",
    "    FileNotFoundError\n",
    "        If CSV file not found\n",
    "    \"\"\"\n",
    "    split_paths = {\n",
    "        \"train\": train_csv,\n",
    "        \"val\": val_csv,\n",
    "        \"test\": test_csv\n",
    "    }\n",
    "\n",
    "    if split not in split_paths:\n",
    "        raise ValueError(f\"Invalid split: {split}. Must be one of {list(split_paths.keys())}\")\n",
    "\n",
    "    csv_path = split_paths[split]\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
    "    \n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "print(\"✓ Function 'get_split_data' defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2: preprocess_row()\n",
    "\n",
    "This function validates and preprocesses each sample before feature extraction.\n",
    "\n",
    "We'll show both a basic version and an advanced version with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Basic 'preprocess_row' defined\n"
     ]
    }
   ],
   "source": [
    "# BASIC VERSION (no validation)\n",
    "def preprocess_row(row):\n",
    "    \"\"\"Pass through - no preprocessing needed.\"\"\"\n",
    "    return row\n",
    "\n",
    "print(\"✓ Basic 'preprocess_row' defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 3: extract_features()\n",
    "\n",
    "This is the main entry point that orchestrates feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Function 'extract_features' defined\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from functools import partial\n",
    "from tumorimagingbench.evaluation.base_feature_extractor import extract_all_features, save_features\n",
    "\n",
    "def extract_features(output_path, train_csv, val_csv, test_csv, model_names=None):\n",
    "    \"\"\"\n",
    "    Extract features for all models.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_path : str\n",
    "        Where to save extracted features (pickle file)\n",
    "    train_csv : str\n",
    "        Path to training annotations CSV\n",
    "    val_csv : str\n",
    "        Path to validation annotations CSV\n",
    "    test_csv : str\n",
    "        Path to test annotations CSV\n",
    "    model_names : list of str, optional\n",
    "        Specific models to extract. If None, extracts all available models.\n",
    "        Example: ['DummyResNetExtractor', 'FMCIBExtractor']\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Saves results to output_path\n",
    "\n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> # Extract all models\n",
    "    >>> extract_features('features/my_dataset.pkl',\n",
    "    ...                   'data/train.csv', 'data/val.csv', 'data/test.csv')\n",
    "    >>>\n",
    "    >>> # Extract specific models\n",
    "    >>> extract_features('features/my_dataset.pkl',\n",
    "    ...                   'data/train.csv', 'data/val.csv', 'data/test.csv',\n",
    "    ...                   model_names=['DummyResNetExtractor'])\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TumorImagingBench Feature Extraction\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Create a partial function that binds CSV paths\n",
    "    get_split_fn = partial(get_split_data, train_csv=train_csv, val_csv=val_csv, test_csv=test_csv)\n",
    "\n",
    "    # Extract features for all models\n",
    "    features = extract_all_features(get_split_fn, preprocess_row, model_names=model_names)\n",
    "\n",
    "    # Save results to disk\n",
    "    save_features(features, output_path)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"✓ Feature extraction completed successfully\")\n",
    "    print(f\"✓ Results saved to {output_path}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(\"✓ Function 'extract_features' defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Complete Example\n",
    "\n",
    "Let's demonstrate the complete workflow with a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Example CSVs with Dummy Data\n",
    "\n",
    "We'll create minimal example CSV files to demonstrate the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created example data directory: /tmp/tumor_imaging_example/data/eval/my_dataset/images\n",
      "Example directory structure:\n",
      "  /tmp/tumor_imaging_example/\n",
      "  ├── data/eval/my_dataset/\n",
      "  │   ├── train.csv\n",
      "  │   ├── val.csv\n",
      "  │   ├── test.csv\n",
      "  │   └── images/\n",
      "  │       ├── scan_001.nii.gz\n",
      "  │       ├── scan_002.nii.gz\n",
      "  │       └── scan_003.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# Create example data directory\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a temporary directory for our example\n",
    "example_dir = Path(tempfile.gettempdir()) / \"tumor_imaging_example\"\n",
    "example_dir.mkdir(exist_ok=True)\n",
    "\n",
    "data_dir = example_dir / \"data\" / \"eval\" / \"my_dataset\" / \"images\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Created example data directory: {data_dir}\")\n",
    "print(f\"Example directory structure:\")\n",
    "print(f\"  {example_dir}/\")\n",
    "print(f\"  ├── data/eval/my_dataset/\")\n",
    "print(f\"  │   ├── train.csv\")\n",
    "print(f\"  │   ├── val.csv\")\n",
    "print(f\"  │   ├── test.csv\")\n",
    "print(f\"  │   └── images/\")\n",
    "print(f\"  │       ├── scan_001.nii.gz\")\n",
    "print(f\"  │       ├── scan_002.nii.gz\")\n",
    "print(f\"  │       └── scan_003.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created /tmp/tumor_imaging_example/data/eval/my_dataset/images/scan_001.nii.gz\n",
      "✓ Created /tmp/tumor_imaging_example/data/eval/my_dataset/images/scan_002.nii.gz\n",
      "✓ Created /tmp/tumor_imaging_example/data/eval/my_dataset/images/scan_003.nii.gz\n",
      "\n",
      "✓ All dummy NIFTI files created\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "# Create dummy NIFTI files\n",
    "for i in range(1, 4):\n",
    "    # Create dummy image data (32x32x32 volume)\n",
    "    data = np.random.rand(32, 32, 32).astype(np.float32)\n",
    "    \n",
    "    # Create affine matrix (identity with 1mm spacing)\n",
    "    affine = np.eye(4)\n",
    "    affine[0, 0] = 1.0  # X spacing\n",
    "    affine[1, 1] = 1.0  # Y spacing\n",
    "    affine[2, 2] = 1.0  # Z spacing\n",
    "    \n",
    "    # Create NIFTI image\n",
    "    img = nib.Nifti1Image(data, affine)\n",
    "    \n",
    "    # Save\n",
    "    filepath = data_dir / f\"scan_{i:03d}.nii.gz\"\n",
    "    nib.save(img, filepath)\n",
    "    print(f\"✓ Created {filepath}\")\n",
    "\n",
    "print(\"\\n✓ All dummy NIFTI files created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created /tmp/tumor_imaging_example/data/eval/my_dataset/train.csv\n",
      "✓ Created /tmp/tumor_imaging_example/data/eval/my_dataset/val.csv\n",
      "✓ Created /tmp/tumor_imaging_example/data/eval/my_dataset/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Create example CSV files\n",
    "train_data = {\n",
    "    'image_path': [\n",
    "        str(data_dir / \"scan_001.nii.gz\"),\n",
    "    ],\n",
    "    'coordX': [15.5],\n",
    "    'coordY': [16.0],\n",
    "    'coordZ': [16.5],\n",
    "    'label': [0]\n",
    "}\n",
    "\n",
    "val_data = {\n",
    "    'image_path': [\n",
    "        str(data_dir / \"scan_002.nii.gz\"),\n",
    "    ],\n",
    "    'coordX': [14.5],\n",
    "    'coordY': [15.0],\n",
    "    'coordZ': [15.5],\n",
    "    'label': [1]\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    'image_path': [\n",
    "        str(data_dir / \"scan_003.nii.gz\"),\n",
    "    ],\n",
    "    'coordX': [16.5],\n",
    "    'coordY': [17.0],\n",
    "    'coordZ': [17.5],\n",
    "    'label': [0]\n",
    "}\n",
    "\n",
    "# Save CSVs\n",
    "train_csv_path = example_dir / \"data\" / \"eval\" / \"my_dataset\" / \"train.csv\"\n",
    "val_csv_path = example_dir / \"data\" / \"eval\" / \"my_dataset\" / \"val.csv\"\n",
    "test_csv_path = example_dir / \"data\" / \"eval\" / \"my_dataset\" / \"test.csv\"\n",
    "\n",
    "pd.DataFrame(train_data).to_csv(train_csv_path, index=False)\n",
    "pd.DataFrame(val_data).to_csv(val_csv_path, index=False)\n",
    "pd.DataFrame(test_data).to_csv(test_csv_path, index=False)\n",
    "\n",
    "print(f\"✓ Created {train_csv_path}\")\n",
    "print(f\"✓ Created {val_csv_path}\")\n",
    "print(f\"✓ Created {test_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train CSV:\n",
      "                                          image_path  coordX  coordY  coordZ  \\\n",
      "0  /tmp/tumor_imaging_example/data/eval/my_datase...    15.5    16.0    16.5   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "\n",
      "Val CSV:\n",
      "                                          image_path  coordX  coordY  coordZ  \\\n",
      "0  /tmp/tumor_imaging_example/data/eval/my_datase...    14.5    15.0    15.5   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "\n",
      "Test CSV:\n",
      "                                          image_path  coordX  coordY  coordZ  \\\n",
      "0  /tmp/tumor_imaging_example/data/eval/my_datase...    16.5    17.0    17.5   \n",
      "\n",
      "   label  \n",
      "0      0  \n"
     ]
    }
   ],
   "source": [
    "# Verify CSV contents\n",
    "print(\"\\nTrain CSV:\")\n",
    "print(pd.read_csv(train_csv_path))\n",
    "print(\"\\nVal CSV:\")\n",
    "print(pd.read_csv(val_csv_path))\n",
    "print(\"\\nTest CSV:\")\n",
    "print(pd.read_csv(test_csv_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Test the get_split_data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train split:\n",
      "                                          image_path  coordX  coordY  coordZ  \\\n",
      "0  /tmp/tumor_imaging_example/data/eval/my_datase...    15.5    16.0    16.5   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "\n",
      "Shape: (1, 5)\n",
      "Columns: ['image_path', 'coordX', 'coordY', 'coordZ', 'label']\n"
     ]
    }
   ],
   "source": [
    "# Test loading train split\n",
    "train_df = get_split_data('train', str(train_csv_path), str(val_csv_path), str(test_csv_path))\n",
    "print(\"Loaded train split:\")\n",
    "print(train_df)\n",
    "print(f\"\\nShape: {train_df.shape}\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded val split:\n",
      "                                          image_path  coordX  coordY  coordZ  \\\n",
      "0  /tmp/tumor_imaging_example/data/eval/my_datase...    14.5    15.0    15.5   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "\n",
      "Loaded test split:\n",
      "                                          image_path  coordX  coordY  coordZ  \\\n",
      "0  /tmp/tumor_imaging_example/data/eval/my_datase...    16.5    17.0    17.5   \n",
      "\n",
      "   label  \n",
      "0      0  \n"
     ]
    }
   ],
   "source": [
    "# Test loading val split\n",
    "val_df = get_split_data('val', str(train_csv_path), str(val_csv_path), str(test_csv_path))\n",
    "print(\"Loaded val split:\")\n",
    "print(val_df)\n",
    "\n",
    "# Test loading test split\n",
    "test_df = get_split_data('test', str(train_csv_path), str(val_csv_path), str(test_csv_path))\n",
    "print(\"\\nLoaded test split:\")\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Test the preprocess_row Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original row:\n",
      "image_path    /tmp/tumor_imaging_example/data/eval/my_datase...\n",
      "coordX                                                     15.5\n",
      "coordY                                                     16.0\n",
      "coordZ                                                     16.5\n",
      "label                                                         0\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Preprocessed row:\n",
      "image_path    /tmp/tumor_imaging_example/data/eval/my_datase...\n",
      "coordX                                                     15.5\n",
      "coordY                                                     16.0\n",
      "coordZ                                                     16.5\n",
      "label                                                         0\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Preprocessing successful: True\n"
     ]
    }
   ],
   "source": [
    "# Test preprocessing a valid row\n",
    "test_row = train_df.iloc[0]\n",
    "print(\"Original row:\")\n",
    "print(test_row)\n",
    "\n",
    "# Preprocess\n",
    "preprocessed = preprocess_row(test_row)\n",
    "print(\"\\nPreprocessed row:\")\n",
    "print(preprocessed)\n",
    "print(f\"\\nPreprocessing successful: {preprocessed is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Extract Features\n",
    "\n",
    "Now let's actually extract features using our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output path: /tmp/tumor_imaging_example/features/my_dataset.pkl\n",
      "Using dataset CSV files:\n",
      "  Train: /tmp/tumor_imaging_example/data/eval/my_dataset/train.csv\n",
      "  Val:   /tmp/tumor_imaging_example/data/eval/my_dataset/val.csv\n",
      "  Test:  /tmp/tumor_imaging_example/data/eval/my_dataset/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = example_dir / \"features\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_path = str(output_dir / \"my_dataset.pkl\")\n",
    "\n",
    "print(f\"Output path: {output_path}\")\n",
    "print(f\"Using dataset CSV files:\")\n",
    "print(f\"  Train: {train_csv_path}\")\n",
    "print(f\"  Val:   {val_csv_path}\")\n",
    "print(f\"  Test:  {test_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TumorImagingBench Feature Extraction\n",
      "======================================================================\n",
      "[<class 'tumorimagingbench.models.dummy_resnet.DummyResNetExtractor'>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.spatial.dictionary Orientationd.__init__:labels: Current default value of argument `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` was changed in version None from `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` to `labels=None`. Default value changed to None meaning that the transform now uses the 'space' of a meta-tensor, if applicable, to determine appropriate axis labels.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing DummyResNetExtractor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.58s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /tmp/tumor_imaging_example/features/my_dataset.pkl\n",
      "======================================================================\n",
      "✓ Feature extraction completed successfully\n",
      "✓ Results saved to /tmp/tumor_imaging_example/features/my_dataset.pkl\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Extract features using only DummyResNetExtractor for speed\n",
    "# In production, you would extract all models by omitting model_names\n",
    "\n",
    "try:\n",
    "    extract_features(\n",
    "        output_path=output_path,\n",
    "        train_csv=str(train_csv_path),\n",
    "        val_csv=str(val_csv_path),\n",
    "        test_csv=str(test_csv_path),\n",
    "        model_names=['DummyResNetExtractor']  # Use specific model for demo\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during feature extraction: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Load and Inspect Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features structure:\n",
      "Models: ['DummyResNetExtractor']\n",
      "--------------------------------\n",
      "train\n",
      "(1, 512)\n",
      "val\n",
      "(1, 512)\n",
      "test\n",
      "(1, 512)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the extracted features\n",
    "with open(output_path, 'rb') as f:\n",
    "    features = pickle.load(f)\n",
    "\n",
    "print(\"Extracted features structure:\")\n",
    "print(f\"Models: {list(features.keys())}\")\n",
    "print(\"--------------------------------\")\n",
    "for model_name in features.keys():\n",
    "    model_features = features[model_name]\n",
    "    for split_name, split_features in model_features.items():\n",
    "        print(split_name)\n",
    "        print(np.vstack([v[\"feature\"] for v in split_features]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "### Example 1: Extract All Models (Default)\n",
    "\n",
    "```python\n",
    "from src.tumorimagingbench.evaluation.my_dataset_feature_extractor import extract_features\n",
    "\n",
    "extract_features(\n",
    "    output_path='features/my_dataset.pkl',\n",
    "    train_csv='data/eval/my_dataset/train.csv',\n",
    "    val_csv='data/eval/my_dataset/val.csv',\n",
    "    test_csv='data/eval/my_dataset/test.csv'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Extract Specific Models Only\n",
    "\n",
    "```python\n",
    "extract_features(\n",
    "    output_path='features/my_dataset.pkl',\n",
    "    train_csv='data/eval/my_dataset/train.csv',\n",
    "    val_csv='data/eval/my_dataset/val.csv',\n",
    "    test_csv='data/eval/my_dataset/test.csv',\n",
    "    model_names=['DummyResNetExtractor', 'FMCIBExtractor']\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Command-Line Usage\n",
    "\n",
    "Create a file `src/tumorimagingbench/evaluation/my_dataset_feature_extractor.py` and add this code:\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Extract features for My Dataset\",\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "EXAMPLES:\n",
    "  # Extract all models\n",
    "  python my_dataset_feature_extractor.py \\\n",
    "    --output features/my_dataset.pkl \\\n",
    "    --train-csv /path/to/train.csv \\\n",
    "    --val-csv /path/to/val.csv \\\n",
    "    --test-csv /path/to/test.csv\n",
    "\n",
    "  # Extract specific models\n",
    "  python my_dataset_feature_extractor.py \\\n",
    "    --output features/my_dataset.pkl \\\n",
    "    --train-csv /path/to/train.csv \\\n",
    "    --val-csv /path/to/val.csv \\\n",
    "    --test-csv /path/to/test.csv \\\n",
    "    --models DummyResNetExtractor FMCIBExtractor\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=str,\n",
    "        default=\"features/my_dataset.pkl\",\n",
    "        help=\"Path where to save extracted features\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train-csv\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to training annotations CSV\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--val-csv\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to validation annotations CSV\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-csv\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to test annotations CSV\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--models\",\n",
    "        type=str,\n",
    "        nargs=\"+\",\n",
    "        default=None,\n",
    "        help=\"Specific models to extract (space-separated)\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    extract_features(args.output, args.train_csv, args.val_csv, args.test_csv, args.models)\n",
    "```\n",
    "\n",
    "Then run:\n",
    "\n",
    "```bash\n",
    "cd src/tumorimagingbench/evaluation\n",
    "python my_dataset_feature_extractor.py \\\n",
    "  --output features/my_dataset.pkl \\\n",
    "  --train-csv /path/to/train.csv \\\n",
    "  --val-csv /path/to/val.csv \\\n",
    "  --test-csv /path/to/test.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Implementations\n",
    "\n",
    "For reference, see these existing extractors in the codebase:\n",
    "\n",
    "- **Simple example**: `dummy_dataset_feature_extractor.py` - Basic template with minimal preprocessing\n",
    "- **Real dataset**: `luna_feature_extractor.py` - LUNA16 lung nodule dataset\n",
    "- **Complex example**: `nsclc_radiomics_feature_extractor.py` - NSCLC radiomics dataset with labels\n",
    "\n",
    "You can use these as templates for your own dataset extractors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "To add a new dataset to TumorImagingBench:\n",
    "\n",
    "1. **Prepare your data**: Organize images as NIFTI/NRRD files and create CSV files with required columns\n",
    "2. **Create extractor**: Implement `get_split_data()`, `preprocess_row()`, and `extract_features()` functions\n",
    "3. **Run extraction**: Call `extract_features()` or use command-line interface\n",
    "4. **Use results**: Load the pickle file and use features for downstream analysis\n",
    "\n",
    "The framework handles:\n",
    "- Loading all available foundation models\n",
    "- Preprocessing NIFTI images\n",
    "- Extracting features in parallel\n",
    "- Saving results in a unified format\n",
    "\n",
    "You only need to define your dataset-specific logic!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
