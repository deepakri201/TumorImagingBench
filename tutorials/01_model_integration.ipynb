{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Integration Guide\n",
    "\n",
    "How to add a new foundation model to TumorImagingBench.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After this tutorial, you will be able to:\n",
    "- Understand the BaseModel architecture\n",
    "- Implement a new model extractor\n",
    "- Register your model with the framework\n",
    "- Test your model\n",
    "- Use your model in feature extraction pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: BaseModel Architecture\n",
    "\n",
    "### Abstract Base Class\n",
    "\n",
    "All models in TumorImagingBench inherit from `BaseModel` and must implement three abstract methods:\n",
    "\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaseModel(ABC, nn.Module):\n",
    "    \"\"\"Base class for all foundation model feature extractors.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, weights_path: str):\n",
    "        \"\"\"Load model weights from file.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self, x):\n",
    "        \"\"\"Preprocess input data before forward pass.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "### Key Requirements\n",
    "\n",
    "1. **Inherit from BaseModel** - Your class must inherit from `BaseModel`\n",
    "2. **Implement load()** - Load pre-trained weights\n",
    "3. **Implement preprocess()** - Convert input dict to tensor\n",
    "4. **Implement forward()** - Return features as numpy array\n",
    "5. **PyTorch Module** - Inherit from `nn.Module` for GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Input/Output Specifications\n",
    "\n",
    "### Input to preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input format to preprocess():\n",
      "  image_path: /path/to/scan.nii.gz\n",
      "  coordX: 100.5\n",
      "  coordY: 150.3\n",
      "  coordZ: 200.1\n",
      "  label: 1\n",
      "  patient_id: P001\n"
     ]
    }
   ],
   "source": [
    "# Input format to preprocess() method\n",
    "example_input = {\n",
    "    'image_path': '/path/to/scan.nii.gz',  # Path to NIFTI file\n",
    "    'coordX': 100.5,                        # X centroid in physical coordinates (mm)\n",
    "    'coordY': 150.3,                        # Y centroid in physical coordinates (mm)\n",
    "    'coordZ': 200.1,                        # Z centroid in physical coordinates (mm)\n",
    "    # Additional fields are optional but preserved\n",
    "    'label': 1,\n",
    "    'patient_id': 'P001'\n",
    "}\n",
    "\n",
    "print(\"Input format to preprocess():\")\n",
    "for key, value in example_input.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of preprocess() and Input to forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape from preprocess(): torch.Size([1, 1, 48, 48, 48])\n",
      "  - Dimension 0: 1 (or C) = channels (usually 1 for CT)\n",
      "  - Dimension 1-4: (H, W, D) = spatial dimensions (usually 48×48×48)\n",
      "\n",
      "Tensor shape to forward(): torch.Size([4, 1, 48, 48, 48])\n",
      "  - Dimension 0: 4 = batch size\n",
      "  - Dimensions 1-4: (C, H, W, D) = channels and spatial\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Output of preprocess() and input to forward()\n",
    "preprocessed_tensor = torch.randn(1, 1, 48, 48, 48)\n",
    "print(f\"Tensor shape from preprocess(): {preprocessed_tensor.shape}\")\n",
    "print(f\"  - Dimension 0: 1 (or C) = channels (usually 1 for CT)\")\n",
    "print(f\"  - Dimension 1-4: (H, W, D) = spatial dimensions (usually 48×48×48)\")\n",
    "\n",
    "batch_tensor = torch.randn(4, 1, 48, 48, 48)\n",
    "print(f\"\\nTensor shape to forward(): {batch_tensor.shape}\")\n",
    "print(f\"  - Dimension 0: 4 = batch size\")\n",
    "print(f\"  - Dimensions 1-4: (C, H, W, D) = channels and spatial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of forward(): (4, 512)\n",
      "  - Type: <class 'numpy.ndarray'>\n",
      "  - dtype: float32\n",
      "  - Dimension 0: 4 = batch size\n",
      "  - Dimension 1: 512 = feature dimension\n",
      "\n",
      "IMPORTANT: Must be numpy array on CPU, not torch tensor on GPU\n"
     ]
    }
   ],
   "source": [
    "# Output of forward() method\n",
    "batch_size = 4\n",
    "feature_dim = 512\n",
    "\n",
    "features = np.random.randn(batch_size, feature_dim).astype(np.float32)\n",
    "print(f\"Output of forward(): {features.shape}\")\n",
    "print(f\"  - Type: {type(features)}\")\n",
    "print(f\"  - dtype: {features.dtype}\")\n",
    "print(f\"  - Dimension 0: {batch_size} = batch size\")\n",
    "print(f\"  - Dimension 1: {feature_dim} = feature dimension\")\n",
    "print(f\"\\nIMPORTANT: Must be numpy array on CPU, not torch tensor on GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Complete Example - DummyResNetExtractor\n",
    "\n",
    "Let's examine the DummyResNetExtractor which serves as a reference implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: CTClipVitExtractor not available due to missing dependencies: No module named 'transformer_maskgit'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Registered extractor: CTFMExtractor\n",
      "✓ Registered extractor: FMCIBExtractor\n",
      "Warning: MedImageInsightExtractor not available due to missing dependencies: No module named 'MedImageInsight'\n",
      "✓ Registered extractor: MerlinExtractor\n",
      "✓ Registered extractor: ModelsGenExtractor\n",
      "✓ Registered extractor: PASTAExtractor\n",
      "✓ Registered extractor: SUPREMExtractor\n",
      "✓ Registered extractor: VISTA3DExtractor\n",
      "✓ Registered extractor: VocoExtractor\n",
      "✓ Registered extractor: DummyResNetExtractor\n",
      "Model class: DummyResNetExtractor\n",
      "Is PyTorch module: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "required package for reader ITKReader is not installed, or the version doesn't match requirement.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/suraj/Repositories/TumorImagingBench/src')\n",
    "\n",
    "from tumorimagingbench.models import get_extractor\n",
    "\n",
    "# Get the DummyResNetExtractor class\n",
    "DummyResNet = get_extractor('DummyResNetExtractor')\n",
    "\n",
    "# Instantiate\n",
    "model = DummyResNet()\n",
    "print(f\"Model class: {model.__class__.__name__}\")\n",
    "print(f\"Is PyTorch module: {isinstance(model, torch.nn.Module)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Step-by-Step Tutorial - Creating Your Own Model\n",
    "\n",
    "Let's create a simplified custom model step by step.\n",
    "\n",
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tumorimagingbench.models import BaseModel\n",
    "from tumorimagingbench.models.utils import get_transforms\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model class defined\n"
     ]
    }
   ],
   "source": [
    "class SimpleModelExtractor(BaseModel):\n",
    "    \"\"\"\n",
    "    A simple model for educational purposes.\n",
    "    \n",
    "    This model demonstrates:\n",
    "    - Inheriting from BaseModel\n",
    "    - Implementing required abstract methods\n",
    "    - Using MONAI preprocessing\n",
    "    - Extracting fixed-size features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the model.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simple feature extractor\n",
    "        # Input: (1, 48, 48, 48)\n",
    "        # Output: (256,)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv3d(1, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(2),\n",
    "            nn.Conv3d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "        )\n",
    "        \n",
    "        # Linear projection to feature space\n",
    "        self.head = nn.Linear(16, 256)\n",
    "        \n",
    "        # Preprocessing pipeline\n",
    "        self.transforms = get_transforms(\n",
    "            orient=\"RAS\",\n",
    "            scale_range=(-1024, 2048),\n",
    "            spatial_size=(48, 48, 48),\n",
    "            spacing=(1, 1, 1),\n",
    "        )\n",
    "        \n",
    "        self.feature_dim = 256\n",
    "\n",
    "    def load(self, weights_path=None):\n",
    "        \"\"\"Load model weights.\"\"\"\n",
    "        # In this simple example, we skip weight loading\n",
    "        # In practice, load from weights_path if provided\n",
    "        self.eval()\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        \"\"\"Preprocess input.\"\"\"\n",
    "        return self.transforms(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract features.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Backbone\n",
    "            features = self.backbone(x)  # (batch, 16, 1, 1, 1)\n",
    "            features = features.view(features.shape[0], -1)  # (batch, 16)\n",
    "            \n",
    "            # Head\n",
    "            features = self.head(features)  # (batch, 256)\n",
    "            \n",
    "            # Move to CPU and convert to numpy\n",
    "            return features.cpu().numpy()\n",
    "\n",
    "print(\"✓ Model class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Register the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Registered extractor: SimpleModelExtractor\n",
      "Registered models: ['CTFMExtractor', 'FMCIBExtractor', 'MerlinExtractor', 'ModelsGenExtractor', 'PASTAExtractor', 'SUPREMExtractor', 'VISTA3DExtractor', 'VocoExtractor', 'DummyResNetExtractor', 'SimpleModelExtractor']\n",
      "✓ Model successfully registered\n"
     ]
    }
   ],
   "source": [
    "from tumorimagingbench.models import register_extractor, get_available_extractors\n",
    "\n",
    "# Register the model\n",
    "register_extractor('SimpleModelExtractor', SimpleModelExtractor)\n",
    "\n",
    "# Verify it's registered\n",
    "available = get_available_extractors()\n",
    "print(f\"Registered models: {available}\")\n",
    "assert 'SimpleModelExtractor' in available, \"Model not registered!\"\n",
    "print(\"✓ Model successfully registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Test Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model instantiated: SimpleModelExtractor\n",
      "✓ Weights loaded\n",
      "✓ Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "# Instantiate your model\n",
    "model = SimpleModelExtractor()\n",
    "print(f\"✓ Model instantiated: {model.__class__.__name__}\")\n",
    "\n",
    "# Load weights\n",
    "model.load()\n",
    "print(\"✓ Weights loaded\")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "print(f\"✓ Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 1, 48, 48, 48])\n",
      "Input device: cuda:0\n",
      "\n",
      "Output shape: (4, 256)\n",
      "Output type: <class 'numpy.ndarray'>\n",
      "Output dtype: float32\n",
      "\n",
      "Feature statistics:\n",
      "  Mean: 0.010754\n",
      "  Std: 0.229787\n",
      "  Min: -0.633219\n",
      "  Max: 0.545278\n"
     ]
    }
   ],
   "source": [
    "# Create dummy input\n",
    "batch_size = 4\n",
    "dummy_input = torch.randn(batch_size, 1, 48, 48, 48, device=device)\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Input device: {dummy_input.device}\")\n",
    "\n",
    "# Extract features\n",
    "features = model.forward(dummy_input)\n",
    "\n",
    "print(f\"\\nOutput shape: {features.shape}\")\n",
    "print(f\"Output type: {type(features)}\")\n",
    "print(f\"Output dtype: {features.dtype}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"  Mean: {features.mean():.6f}\")\n",
    "print(f\"  Std: {features.std():.6f}\")\n",
    "print(f\"  Min: {features.min():.6f}\")\n",
    "print(f\"  Max: {features.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Verify Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output format verification:\n",
      "  ✓ Output is numpy array\n",
      "  ✓ Batch size preserved (4)\n",
      "  ✓ Feature dimension correct (256)\n",
      "  ✓ Output dtype is float\n",
      "\n",
      "✓ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Verify output format\n",
    "checks = [\n",
    "    (isinstance(features, np.ndarray), \"Output is numpy array\"),\n",
    "    (features.shape[0] == batch_size, f\"Batch size preserved ({batch_size})\"),\n",
    "    (features.shape[1] == model.feature_dim, f\"Feature dimension correct ({model.feature_dim})\"),\n",
    "    (features.dtype in [np.float32, np.float64], \"Output dtype is float\"),\n",
    "]\n",
    "\n",
    "print(\"Output format verification:\")\n",
    "for passed, description in checks:\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    print(f\"  {status} {description}\")\n",
    "\n",
    "if all(check[0] for check in checks):\n",
    "    print(\"\\n✓ All checks passed!\")\n",
    "else:\n",
    "    print(\"\\n✗ Some checks failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Best Practices\n",
    "\n",
    "### Best Practice 1: Comprehensive Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Well-documented model extractor.\n",
      "\n",
      "    Architecture:\n",
      "        - Input: 3D volumetric image (1, 48, 48, 48)\n",
      "        - Backbone: ResNet-50 with 3D convolutions\n",
      "        - Output: 2048-dimensional feature vector\n",
      "        - Feature aggregation: Global average pooling\n",
      "\n",
      "    Pre-training:\n",
      "        - Dataset: ImageNet-1k (with 2D-to-3D adaptation)\n",
      "        - Task: Image classification\n",
      "        - Weights: Publicly available from pytorch/vision\n",
      "\n",
      "    Input Requirements:\n",
      "        - NIFTI format (.nii.gz)\n",
      "        - CT intensities: -1024 to 2048 HU\n",
      "        - Physical coordinates in mm\n",
      "\n",
      "    Output:\n",
      "        - Feature dimension: 2048\n",
      "        - Output range: Unbounded (ReLU features)\n",
      "        - Suitable for: Classification, clustering, similarity\n",
      "\n",
      "    Examples:\n",
      "        >>> model = WellDocumentedModel()\n",
      "        >>> model.load()\n",
      "        >>> dummy = torch.randn(1, 1, 48, 48, 48)\n",
      "        >>> features = model.forward(dummy)\n",
      "        >>> print(features.shape)\n",
      "        (1, 2048)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Example of well-documented model\n",
    "class WellDocumentedModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Well-documented model extractor.\n",
    "    \n",
    "    Architecture:\n",
    "        - Input: 3D volumetric image (1, 48, 48, 48)\n",
    "        - Backbone: ResNet-50 with 3D convolutions\n",
    "        - Output: 2048-dimensional feature vector\n",
    "        - Feature aggregation: Global average pooling\n",
    "    \n",
    "    Pre-training:\n",
    "        - Dataset: ImageNet-1k (with 2D-to-3D adaptation)\n",
    "        - Task: Image classification\n",
    "        - Weights: Publicly available from pytorch/vision\n",
    "    \n",
    "    Input Requirements:\n",
    "        - NIFTI format (.nii.gz)\n",
    "        - CT intensities: -1024 to 2048 HU\n",
    "        - Physical coordinates in mm\n",
    "    \n",
    "    Output:\n",
    "        - Feature dimension: 2048\n",
    "        - Output range: Unbounded (ReLU features)\n",
    "        - Suitable for: Classification, clustering, similarity\n",
    "    \n",
    "    Examples:\n",
    "        >>> model = WellDocumentedModel()\n",
    "        >>> model.load()\n",
    "        >>> dummy = torch.randn(1, 1, 48, 48, 48)\n",
    "        >>> features = model.forward(dummy)\n",
    "        >>> print(features.shape)\n",
    "        (1, 2048)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(WellDocumentedModel.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practice 2: Use Standard Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard preprocessing pipeline created\n",
      "Benefits:\n",
      "  - Consistency across models\n",
      "  - MONAI-based (standard in medical imaging)\n",
      "  - Handles NIFTI loading, orientation, resampling, cropping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n"
     ]
    }
   ],
   "source": [
    "# Use get_transforms() from utils for consistency\n",
    "from tumorimagingbench.models.utils import get_transforms\n",
    "\n",
    "# This ensures consistency across all models\n",
    "transforms = get_transforms(\n",
    "    orient=\"RAS\",              # Standard orientation\n",
    "    scale_range=(-1024, 2048), # CT intensity range\n",
    "    spatial_size=(48, 48, 48), # Standard patch size\n",
    "    spacing=(1, 1, 1),         # Standard spacing\n",
    ")\n",
    "\n",
    "print(\"Standard preprocessing pipeline created\")\n",
    "print(\"Benefits:\")\n",
    "print(\"  - Consistency across models\")\n",
    "print(\"  - MONAI-based (standard in medical imaging)\")\n",
    "print(\"  - Handles NIFTI loading, orientation, resampling, cropping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practice 3: GPU Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient GPU usage:\n",
      "  - Disable gradients: with torch.no_grad()\n",
      "  - Keep data on GPU during computation\n",
      "  - Move to CPU only at the end\n",
      "  - Convert to numpy for downstream tasks\n"
     ]
    }
   ],
   "source": [
    "# Good forward pass implementation\n",
    "def efficient_forward(model, x, device='cuda'):\n",
    "    \"\"\"\n",
    "    Efficient forward pass for GPU inference.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Disable gradients\n",
    "        x = x.to(device)  # Move to GPU\n",
    "        features = model.backbone(x)  # Process on GPU\n",
    "        return features.cpu().numpy()  # Move to CPU only at end\n",
    "\n",
    "print(\"Efficient GPU usage:\")\n",
    "print(\"  - Disable gradients: with torch.no_grad()\")\n",
    "print(\"  - Keep data on GPU during computation\")\n",
    "print(\"  - Move to CPU only at the end\")\n",
    "print(\"  - Convert to numpy for downstream tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Testing Your Model\n",
    "\n",
    "### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Test 1: Instantiation\n",
      "✓ Test 2: Weight loading\n",
      "✓ Test 3: Output shape (2, 256)\n",
      "✓ Test 4: Output is numpy array\n",
      "✓ Test 5: Batch processing\n",
      "\n",
      "✓ All tests passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "In the future `np.bool` will be defined as the corresponding NumPy scalar.\n"
     ]
    }
   ],
   "source": [
    "# Test suite for your model\n",
    "def test_model():\n",
    "    \"\"\"Comprehensive test suite for model.\"\"\"\n",
    "    \n",
    "    # Test 1: Instantiation\n",
    "    model = SimpleModelExtractor()\n",
    "    assert model is not None\n",
    "    print(\"✓ Test 1: Instantiation\")\n",
    "    \n",
    "    # Test 2: Loading\n",
    "    model.load()\n",
    "    print(\"✓ Test 2: Weight loading\")\n",
    "    \n",
    "    # Test 3: Forward shape\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    dummy = torch.randn(2, 1, 48, 48, 48, device=device)\n",
    "    output = model.forward(dummy)\n",
    "    assert output.shape == (2, model.feature_dim)\n",
    "    print(f\"✓ Test 3: Output shape {output.shape}\")\n",
    "    \n",
    "    # Test 4: Output type\n",
    "    assert isinstance(output, np.ndarray)\n",
    "    print(\"✓ Test 4: Output is numpy array\")\n",
    "    \n",
    "    # Test 5: Batch processing\n",
    "    for batch_size in [1, 2, 4, 8]:\n",
    "        dummy = torch.randn(batch_size, 1, 48, 48, 48, device=device)\n",
    "        output = model.forward(dummy)\n",
    "        assert output.shape[0] == batch_size\n",
    "    print(\"✓ Test 5: Batch processing\")\n",
    "    \n",
    "    print(\"\\n✓ All tests passed!\")\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Use in Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved model: SimpleModelExtractor\n"
     ]
    }
   ],
   "source": [
    "# Now your registered model can be used in feature extraction\n",
    "from tumorimagingbench.models import get_extractor\n",
    "\n",
    "# Retrieve your registered model\n",
    "ModelClass = get_extractor('SimpleModelExtractor')\n",
    "print(f\"Retrieved model: {ModelClass.__name__}\")\n",
    "\n",
    "# Use in feature extraction pipeline\n",
    "# python nsclc_radiomics_feature_extractor.py \\\n",
    "#   --output features/nsclc.pkl \\\n",
    "#   --models SimpleModelExtractor DummyResNetExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have learned how to:\n",
    "\n",
    "1. **Understand BaseModel** - The abstract base class all models inherit from\n",
    "2. **Implement a Model** - Create your own model extractor with required methods\n",
    "3. **Register a Model** - Make it available in the framework\n",
    "4. **Test a Model** - Verify correct input/output formats\n",
    "5. **Use in Pipelines** - Integrate with feature extraction\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Check [02_feature_extractor_guide.ipynb](./02_feature_extractor_guide.ipynb) to learn how to add datasets\n",
    "- Review the DummyResNetExtractor: `src/tumorimagingbench/models/dummy_resnet.py`\n",
    "- Explore other models in `src/tumorimagingbench/models/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
